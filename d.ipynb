{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85ac266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train (582 cases)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582/582 [37:45<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing val (65 cases)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [04:10<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test (162 cases)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [10:06<00:00,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Preprocessing complete. Ready for model training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Radiomics: Process NIfTI for Deep Learning Training\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from skimage import measure\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_DIR = pathlib.Path(\"data\")  # folder with nested case folders\n",
    "OUTPUT_DIR = pathlib.Path(\"processed_data\")\n",
    "TARGET_SHAPE = (128, 128, 64)  # standard volume shape\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "EXPORT_PNG = True  # export slices for inspection\n",
    "\n",
    "# Create folders\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    (OUTPUT_DIR / split / \"X\").mkdir(parents=True, exist_ok=True)\n",
    "    (OUTPUT_DIR / split / \"y\").mkdir(parents=True, exist_ok=True)\n",
    "    if EXPORT_PNG:\n",
    "        (OUTPUT_DIR / split / \"png\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_nii_image(path):\n",
    "    image = sitk.ReadImage(str(path))\n",
    "    array = sitk.GetArrayFromImage(image)  # shape: (Z, Y, X)\n",
    "    return array.astype(np.float32)\n",
    "\n",
    "def normalize_volume(volume):\n",
    "    volume = np.clip(volume, np.percentile(volume, 1), np.percentile(volume, 99))\n",
    "    volume -= volume.min()\n",
    "    volume /= volume.max() + 1e-8\n",
    "    return volume\n",
    "\n",
    "def resize_volume(volume, target_shape):\n",
    "    return resize(volume, target_shape, mode='constant', preserve_range=True)\n",
    "\n",
    "def save_middle_slices(image, label, case_name, output_folder):\n",
    "    z_mid = image.shape[2] // 2\n",
    "    y_mid = image.shape[1] // 2\n",
    "    x_mid = image.shape[0] // 2\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 6))\n",
    "    axs[0, 0].imshow(image[x_mid, :, :], cmap='gray'); axs[0, 0].set_title('X slice')\n",
    "    axs[0, 1].imshow(image[:, y_mid, :], cmap='gray'); axs[0, 1].set_title('Y slice')\n",
    "    axs[0, 2].imshow(image[:, :, z_mid], cmap='gray'); axs[0, 2].set_title('Z slice')\n",
    "\n",
    "    axs[1, 0].imshow(label[x_mid, :, :]); axs[1, 0].set_title('Label X')\n",
    "    axs[1, 1].imshow(label[:, y_mid, :]); axs[1, 1].set_title('Label Y')\n",
    "    axs[1, 2].imshow(label[:, :, z_mid]); axs[1, 2].set_title('Label Z')\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_folder / f\"{case_name}_slices.png\")\n",
    "    plt.close()\n",
    "\n",
    "def visualize_3d(volume, threshold=0.5):\n",
    "    verts, faces, _, _ = measure.marching_cubes(volume, level=threshold)\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    mesh = Poly3DCollection(verts[faces], alpha=0.7)\n",
    "    mesh.set_facecolor([0.5, 0.5, 1])\n",
    "    ax.add_collection3d(mesh)\n",
    "    ax.set_xlim(0, volume.shape[0])\n",
    "    ax.set_ylim(0, volume.shape[1])\n",
    "    ax.set_zlim(0, volume.shape[2])\n",
    "    plt.title(\"3D Segmentation\")\n",
    "    plt.show()\n",
    "\n",
    "def process_case(case_dir):\n",
    "    image_paths = list(case_dir.rglob(\"body-parts.nii.gz\"))  # <- nome real no seu diretório\n",
    "    label_paths = list(case_dir.rglob(\"body-regions.nii.gz\"))\n",
    "    if not image_paths:\n",
    "        print(f\"⚠️ Imagem não encontrada em {case_dir}\")\n",
    "        return None\n",
    "\n",
    "    image = load_nii_image(image_paths[0])\n",
    "    image = normalize_volume(image)\n",
    "    image = resize_volume(image, TARGET_SHAPE)\n",
    "\n",
    "    if label_paths:\n",
    "        label = load_nii_image(label_paths[0])\n",
    "        label = resize_volume(label, TARGET_SHAPE)\n",
    "        label = (label > 0).astype(np.uint8)  # binarize\n",
    "    else:\n",
    "        label = np.zeros(TARGET_SHAPE, dtype=np.uint8)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "# === Find All Valid Cases (folders that contain body-parts.nii.gz) ===\n",
    "all_cases = sorted({p.parent for p in DATA_DIR.rglob(\"body-parts.nii.gz\")})\n",
    "\n",
    "if len(all_cases) == 0:\n",
    "    print(\"❌ Nenhum caso encontrado. Verifique a pasta 'data'.\")\n",
    "    train, val, test = [], [], []\n",
    "elif len(all_cases) == 1:\n",
    "    train, val, test = all_cases, [], []\n",
    "    print(\"⚠️ Apenas 1 caso encontrado: todos os dados serão usados como treino.\")\n",
    "else:\n",
    "    test_size = TEST_SIZE if len(all_cases) > 4 else max(1 / len(all_cases), 0.2)\n",
    "    val_size = VAL_SIZE if len(all_cases) > 4 else 0\n",
    "    train_val, test = train_test_split(all_cases, test_size=test_size, random_state=42)\n",
    "    if len(train_val) > 1 and val_size > 0:\n",
    "        train, val = train_test_split(train_val, test_size=val_size, random_state=42)\n",
    "    else:\n",
    "        train, val = train_val, []\n",
    "\n",
    "splits = [(train, \"train\"), (val, \"val\"), (test, \"test\")]\n",
    "\n",
    "for split_cases, split_name in splits:\n",
    "    if not split_cases:\n",
    "        continue\n",
    "    print(f\"\\nProcessing {split_name} ({len(split_cases)} cases)...\")\n",
    "    for case_dir in tqdm(split_cases):\n",
    "        result = process_case(case_dir)\n",
    "        if result is None:\n",
    "            continue\n",
    "        image, label = result\n",
    "        np.save(OUTPUT_DIR / split_name / \"X\" / f\"{case_dir.name}.npy\", image)\n",
    "        np.save(OUTPUT_DIR / split_name / \"y\" / f\"{case_dir.name}.npy\", label)\n",
    "\n",
    "        if EXPORT_PNG:\n",
    "            save_middle_slices(image, label, case_dir.name, OUTPUT_DIR / split_name / \"png\")\n",
    "\n",
    "print(\"\\n✅ Preprocessing complete. Ready for model training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be82f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 8/291 [04:23<2:33:45, 32.60s/it]"
     ]
    }
   ],
   "source": [
    "# U-Net 3D Training for Radiomics Segmentation\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "# ===================\n",
    "# Configuration\n",
    "# ===================\n",
    "DATA_DIR = Path(\"processed_data\")\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT_PATH = Path(\"unet3d_model.pt\")\n",
    "\n",
    "# ===================\n",
    "# Dataset\n",
    "# ===================\n",
    "class NumpyVolumeDataset(Dataset):\n",
    "    def __init__(self, x_dir, y_dir):\n",
    "        self.x_paths = sorted(list(Path(x_dir).glob(\"*.npy\")))\n",
    "        self.y_paths = sorted(list(Path(y_dir).glob(\"*.npy\")))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = np.load(self.x_paths[idx])[None, ...]  # add channel\n",
    "        y = np.load(self.y_paths[idx])[None, ...]  # binary mask\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# ===================\n",
    "# U-Net 3D Model\n",
    "# ===================\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.enc1 = DoubleConv(in_channels, 32)\n",
    "        self.enc2 = DoubleConv(32, 64)\n",
    "        self.enc3 = DoubleConv(64, 128)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.up2 = nn.ConvTranspose3d(128, 64, 2, stride=2)\n",
    "        self.dec2 = DoubleConv(128, 64)\n",
    "        self.up1 = nn.ConvTranspose3d(64, 32, 2, stride=2)\n",
    "        self.dec1 = DoubleConv(64, 32)\n",
    "        self.out_conv = nn.Conv3d(32, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(self.pool(x1))\n",
    "        x3 = self.enc3(self.pool(x2))\n",
    "        x = self.up2(x3)\n",
    "        x = self.dec2(torch.cat([x, x2], dim=1))\n",
    "        x = self.up1(x)\n",
    "        x = self.dec1(torch.cat([x, x1], dim=1))\n",
    "        return torch.sigmoid(self.out_conv(x))\n",
    "\n",
    "# ===================\n",
    "# Training Utilities\n",
    "# ===================\n",
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "    intersection = (pred * target).sum(dim=(2, 3, 4))\n",
    "    union = pred.sum(dim=(2, 3, 4)) + target.sum(dim=(2, 3, 4))\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for x, y in tqdm(loader):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    dices = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x)\n",
    "            pred = (pred > 0.5).float()\n",
    "            dice = 1 - dice_loss(pred, y)\n",
    "            dices.append(dice.item())\n",
    "    return np.mean(dices)\n",
    "\n",
    "# ===================\n",
    "# Main Training Loop\n",
    "# ===================\n",
    "train_ds = NumpyVolumeDataset(DATA_DIR / \"train/X\", DATA_DIR / \"train/y\")\n",
    "val_ds = NumpyVolumeDataset(DATA_DIR / \"val/X\", DATA_DIR / \"val/y\")\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)\n",
    "\n",
    "model = UNet3D().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_losses = []\n",
    "val_scores = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, dice_loss)\n",
    "    val_dice = validate(model, val_loader)\n",
    "    print(f\"Train Loss: {loss:.4f} | Val Dice: {val_dice:.4f}\")\n",
    "    train_losses.append(loss)\n",
    "    val_scores.append(val_dice)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
    "print(f\"✅ Modelo salvo em: {CHECKPOINT_PATH}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_scores, label=\"Val Dice\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Curve\")\n",
    "plt.show()\n",
    "\n",
    "# ===================\n",
    "# Prediction on Test Set + Save\n",
    "# ===================\n",
    "test_ds = NumpyVolumeDataset(DATA_DIR / \"test/X\", DATA_DIR / \"test/y\")\n",
    "test_loader = DataLoader(test_ds, batch_size=1)\n",
    "model.eval()\n",
    "\n",
    "PRED_DIR = DATA_DIR / \"predictions\"\n",
    "PRED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x, _) in enumerate(test_loader):\n",
    "        x = x.to(DEVICE)\n",
    "        pred = model(x)\n",
    "        pred = (pred > 0.5).float().cpu().numpy()[0, 0]\n",
    "        nib.save(nib.Nifti1Image(pred, affine=np.eye(4)), PRED_DIR / f\"pred_{i:03d}.nii.gz\")\n",
    "\n",
    "print(f\"🎯 Predições salvas em: {PRED_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
